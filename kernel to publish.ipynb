{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "82c39e41e27410b00335d70daa2361c762c44a20"
      },
      "cell_type": "code",
      "source": "base_path=\"../input/\"\ntrain_file = base_path+\"train_v2.csv\"\ntest_file = base_path+\"test_v2.csv\"\nmy_submission_file = base_path+\"mysub.csv\"",
      "execution_count": 45,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c9e3f9851dec43de4074b064cf5bcbdb7880de0f"
      },
      "cell_type": "markdown",
      "source": "## Generic"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b286e88e1ccd665b2bd9c4796c4f953dab7081a"
      },
      "cell_type": "code",
      "source": "#mathplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import pylab\nimport matplotlib as mpl\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n\n#math\nimport math\n\n#numpy\nfrom numpy import arange,array,ones\nimport numpy as np\n\n#scipy\nfrom scipy import stats\nimport scipy\n\n#csv\nimport csv, json\n\n#pandas\nimport pandas as pd\nimport pandas\nfrom pandas.plotting import scatter_matrix\nfrom pandas.io.json import json_normalize\n\n#seaborn\nimport seaborn as sns\n\n#options\nget_ipython().run_line_magic('matplotlib', 'notebook')\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n#lgb\nimport lightgbm as lgb\n\n#sklearn\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.preprocessing import OneHotEncoder\n\n#os\nimport os\nimport datetime\nimport sys, time\n\ndef common_feats(train, test):\n    return list(set(test.columns).intersection(set(train.columns)))\n\n\ndef drop_uncommons(train, test, exceptions):\n    commons = common_feats(train, test)\n\n    for col in train.columns:\n        if (col not in commons) & (col not in exceptions):\n            train = train.drop(col, axis=1)\n\n    for col in test.columns:\n        if (col not in commons) & (col not in exceptions):\n            test = test.drop(col, axis=1)\n\n    return train, test\n\n\nclass toolbar:\n\n    def __init__(self, length=10):\n        self.length = length\n        self.steps = 1\n\n    def set_len(self, new):\n        self.length = new\n\n    def prin(self):\n        if (self.steps <= self.length):\n            str1 = \"[\"\n            for i in range(self.steps):\n                str1 += '*'\n            for i in range(self.length - self.steps):\n                str1 += ' '\n            str1 += \"]\"\n            sys.stdout.write('\\r' * self.length + str(str1))\n\n            self.steps += 1\n\n    def update(self):\n        if (self.steps < self.length):\n            self.steps += 1\n\n\n#tipi numerici\ntipi_num = [np.int64,float,int]\n\ndef find_num_cols(df,exceptions,cats,nums):\n    vet = []\n    for col in df.columns:\n        try:\n            if (type(eval(df[col][0])) in tipi_num) & (col not in cats) & (col not in nums) & (\n                    df[col].nunique() > 10) & (col not in exceptions):\n                #print(\"aggiugngo ai num_cols\", col)\n                vet += [col]\n        except:\n            try:\n                if (type(df[col][0]) in tipi_num) & (col not in cats) & (col not in nums) & (\n                        df[col].nunique() > 10) & (col not in exceptions):\n                    #print(\"aggiugngo ai num_cols\", col)\n                    vet += [col]\n            except:\n                pass\n\n    vet += nums\n    return vet\n\n\ndef find_cat_cols(df,exceptions,nums):\n    return list( (set(df.columns).difference(set(nums))).difference(set(exceptions))   )\n\nclass game:\n\n    def __init__(self):\n        self.ph = pd.DataFrame([[\"Florida\", 2], [\"Florida\", 8], [\"Arizona\", 4]], columns=[\"a\", \"b\"])\n\n    def view(self):\n        self.ph.head()\n\n    def applica(self, fun):\n        return fun(self.ph)\n\n    def restore(self):\n        self.ph = pd.DataFrame([[\"Florida\", 2], [\"Florida\", 8], [\"Arizona\", 4]], columns=[\"a\", \"b\"])\n        return self.ph\n\n    def mean(self):\n        return self.ph.groupby(\"a\").mean()\n\n    def get(self):\n        return self.ph\n        \ndef stringify_cats(df,cats):\n    df[cats] = df[cats].astype(str)\n    return df\n\ndef drop_exceeding(train,test,max_new_feat,cats,target):\n    da_droppare=[]\n\n    for col in cats:\n        if (train[col].nunique()>max_new_feat) & (col not in target):\n            da_droppare+=[col]\n            train=train.drop(col,axis=1)\n\n    for col in da_droppare:\n        test=test.drop(col,axis=1)\n\n    new=[]\n    for col in cats:\n        if col not in da_droppare:\n            new += [col]\n\n    cats = new\n    \n    return train,test,cats\n\n# check_coherence between train_df cat_cols num_cols test_df\ndef cc(train_df,test_df,cats,nums):\n    if len(list(set(cats).intersection(set(nums)))) > 0:\n        raise Exception(\"intersection not empty between cats & nums\")\n\n    for col in cats:\n        if col not in train_df.columns:\n            raise Exception(\"incohererence between cat_cols and train_df.columns with col {}\".format(col))\n\n    for col in nums:\n        if col not in train_df.columns:\n            raise Exception(\"incohererence between num_cols and train_df.columns with col {}\".format(col))\n\n    for col in cats:\n        if col not in test_df.columns:\n            raise Exception(\"incohererence between cat_cols and test_df.columns with col {}\".format(col))\n\n    for col in nums:\n        if (col not in test_df.columns) & (col != \"totals.totalTransactionRevenue\"):\n            raise Exception(\"incohererence between num_cols and test_df.columns with col {}\".format(col))\n            \n    if \"totals.totalTransactionRevenue\" not in nums:\n        raise Exception(\"incohererence: nums is corrupted\")\n\n    if \"totals.totalTransactionRevenue\" not in train_df.columns:\n        raise Exception(\"incohererence: transactionRevenue not in train_df.columns\")\n\n    if (\"fullVisitorId\" not in train_df.columns) | (\"fullVisitorId\" not in test_df.columns):\n        raise Exception(\"incohererence: fullVisitorId not in train_df.columns or test_df.columns\")\n        \n    if len(list(set(cats).intersection(set(nums)))):\n        raise Exception(\"intersection not-empty\")\n        \n    for col in train_df.columns:\n        if type(col)!=type(\"\"):\n            raise Exception(\"uncorrect type in cols names of train\")\n            \n    for col in test_df.columns:\n        if type(col)!=type(\"\"):\n            raise Exception(\"uncorrect type in cols names of test\")\n            \n    if \"totals.totalTransactionRevenue\" in test_df.columns:\n        raise Exception(\"target dont have to stay in test dataset\")\n        \n        \n        \ndef norm(l):\n    return (l-min(l))/(max(l) - min(l))",
      "execution_count": 46,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "888c4ef51bfe4125073877a814fabc1f175a8afb"
      },
      "cell_type": "markdown",
      "source": "## Preprocessing"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7bce0c624053a202bf827354902dae09607ce31c"
      },
      "cell_type": "code",
      "source": "#mathplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import pylab\nimport matplotlib as mpl\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n\n#math\nimport math\n\n#numpy\nfrom numpy import arange,array,ones\nimport numpy as np\n\n#scipy\nfrom scipy import stats\nimport scipy\n\n#csv\nimport csv, json\n\n#pandas\nimport pandas as pd\nimport pandas\nfrom pandas.plotting import scatter_matrix\nfrom pandas.io.json import json_normalize\n\n#seaborn\nimport seaborn as sns\n\n#options\nget_ipython().run_line_magic('matplotlib', 'notebook')\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n#lgb\nimport lightgbm as lgb\n\n#sklearn\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.preprocessing import OneHotEncoder\n\n#os\nimport os\nimport datetime\nimport sys, time\n\ndef flatten(twoDim):\n    support = []\n    for new_feats in twoDim:\n        for feat in new_feats:\n            support += [feat]\n    return support\n\ndef into_dataframe(col_names, matr):\n    new_data = pd.DataFrame()\n    for i, name in enumerate(col_names):\n        new_data[name] = pd.Series(matr[:, i])\n\n    return new_data\n\ndef drop_cols(df, not_drop):\n    return df[not_drop]\n\ndef encode_cats(train, test, cats):\n    t = toolbar(11)\n\n    t.prin()\n    # creo encoder\n    oh_enc = OneHotEncoder(sparse=False)\n\n    t.prin()\n    # encodo e prendo i nomi vecchi\n    new_cols_train = oh_enc.fit_transform(array(train[cats]))\n    feat_train = oh_enc.categories_\n\n    t.prin()\n    # anche per il test\n    new_cols_test = oh_enc.fit_transform(array(test[cats]))\n    feat_test = oh_enc.categories_\n\n    t.prin()\n    # i nomi vecchi sono una matr n*n, la rendo n*1\n    feat_train = flatten(feat_train)\n\n    # anche per il test\n    feat_test = flatten(feat_test)\n\n    t.prin()\n    # trasformo le nuove colonne in un dataframe\n    new_train = into_dataframe(feat_train, new_cols_train)\n\n    t.prin()\n    # anche per il test\n    new_test = into_dataframe(feat_test, new_cols_test)\n\n    t.prin()\n    # cerco le colonne uguali nei dataframes\n    commons = common_feats(new_train, new_test)\n\n    t.prin()\n    new_train = drop_cols(new_train, commons)\n    new_test = drop_cols(new_test, commons)\n\n    t.prin()\n    for col in cats:\n        train = train.drop(col, axis=1)\n        test = test.drop(col, axis=1)\n\n    t.prin()\n    cats = commons\n\n    t.prin()\n    train = train.merge(new_train, right_index=True, left_index=True)\n    test = test.merge(new_test, right_index=True, left_index=True)\n\n    t.prin()\n    return train, test, cats\n\n# In[70]:\n\n\ndef reset_index(df, groupper):\n    df[groupper] = df.index\n    df = df.set_index(array([x for x in range(df.shape[0])]))\n\n    return df\n\n\ndef group_me(df,groupper,cats,nums,mode):\n    t = toolbar(6)\n\n    t.prin()\n    # raggruppo per il valore groupper\n\n    # raccolgo con la media per i numeri\n    t.prin()\n    df_num = pd.DataFrame()\n    \n    train=0\n    \n    try:\n        if \"totals.totalTransactionRevenue\" in df.columns:\n            train=1\n    except:\n        pass\n    \n    if train==1:\n        #siamo nel train\n        #print(set(nums).difference(set(nums).intersection(set(df.columns))))\n        df_num[nums] = df.groupby(groupper)[nums].mean()\n    else:\n        #siamo nel test\n        nn = list(set(nums).difference(set([\"totals.totalTransactionRevenue\"])))\n        df_num[nn] = df.groupby(groupper)[nn].mean()\n        \n\n    # raggruppo per la mediana per le categorie\n    t.prin()\n    df_cat = pd.DataFrame()\n    groupped = df.groupby(groupper)\n    \n    if mode == \"mode\":\n        for col in cats:\n            df_cat[col] = groupped[col].agg(lambda x: stats.mode(x)[0][0])\n    if mode == \"mean\":\n        df_cat[cats] = df.groupby(groupper)[cats].mean()\n    if mode == \"median\":\n        df_cat[cats] = df.groupby(groupper)[cats].median()\n    if mode == \"mode_approx\":\n        my_group = df.groupby(groupper)[cats]\n        df_cat[cats] = -(2*my_group.mean() - 3*my_group.median())\n        \n\n    t.prin()\n    # unisco i due nuovi dataframes\n    df = df_num.merge(df_cat, right_index=True, left_index=True)\n\n    t.prin()\n    # df[groupper]=df.index\n    # df = df.set_index(array([x for x in range(df.shape[0])]))\n    df = reset_index(df, groupper)\n\n    t.prin()\n    return df",
      "execution_count": 47,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5f706a52956817465d07cda2fe2029a4571fa2f9"
      },
      "cell_type": "markdown",
      "source": "## Linear Regression"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5701f89f179481295c00c023ee56aabee6d49da4"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef norm(df):\n    return (df-df.min())/(df.max()-df.min())\n\ndef lin(dev,val,test):\n    scaler = MinMaxScaler()\n    dev = scaler.fit_transform(dev)\n    test = scaler.fit_transform(test)\n\n    lr = LinearRegression().fit(np.array(norm(dev)), np.array(val))\n    predictions = lr.predict(np.array(test))\n    predictions[predictions<0] = 0\n    \n    return predictions",
      "execution_count": 48,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e3eaebff35c22d108d2a50c3fc6e855f3becf830"
      },
      "cell_type": "markdown",
      "source": "## Dataset Parsing"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "024513e700e920fca112af8d463f922444b00a0f"
      },
      "cell_type": "code",
      "source": "#mathplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import pylab\nimport matplotlib as mpl\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n\n#math\nimport math\n\n#numpy\nfrom numpy import arange,array,ones\nimport numpy as np\n\n#scipy\nfrom scipy import stats\nimport scipy\n\n#csv\nimport csv, json\n\n#pandas\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\nfrom pandas.io.json import json_normalize\n\n#seaborn\nimport seaborn as sns\n\n#options\nget_ipython().run_line_magic('matplotlib', 'notebook')\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n#lgb\nimport lightgbm as lgb\n\n#sklearn\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.preprocessing import OneHotEncoder\n\n#os\nimport os\nimport datetime\nimport sys, time\n\n# trasforma liste di dizionari in un solo dizionario\ndef unpack(col):\n    mod = []\n    for row in col:\n        try:\n            mod += eval(row)\n        except:\n            mod += row\n\n    return pd.Series(mod)\n\n\ndef correction_nan(x):\n    if type(x) == type(0.0):\n        return {}\n    else:\n        return x\n\n\ndef mappa(fun, ite):\n    return [fun(x) for x in ite]\n\n\n# è una sola la colonna che ricevo e la trasformo in un dict\ndef from_dict_to_frame(df, col):\n    # return pd.DataFrame(df[col])\n\n    df[col] = mappa(correction_nan, df[col])\n\n    new = []\n    for c in df[col]:\n        new += [c]\n    new2 = pd.DataFrame.from_records(new)\n    # corretto new2 -> print(type(new2))\n\n    return new2\n\n\n# trasforma n colonne con un dict all interno in un dizionario che le comprende\ndef unfold(df, cols,target):\n    for col in cols:\n        if col not in target:\n            df = df.merge(from_dict_to_frame(df, col), right_index=True, left_index=True)\n            df = df.drop(col, axis=1)\n\n    return df\n\n\n# trasforma jsons in colonne\ndef load_jsons_as_cols(df, JSON_COLUMNS):\n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n\n    return df\n\n\ndef find_dicts(df,target):\n    dicts_to_dump = []\n    for x in range(len(df.columns)):\n        try:\n            if (type(df.iloc[0, x]) == type({})) & (df.columns[x] not in target):\n                dicts_to_dump += [df.columns[x]]\n        except:\n            try:\n                if (type(eval(df.iloc[0, x])) == type({})) & (df.columns[x] not in target):\n                    dicts_to_dump += [df.columns[x]]\n            except:\n                pass\n    return dicts_to_dump\n\n\ndef find_lists(df,target):\n    lists_to_dump = []\n    for x in range(len(df.columns)):\n        try:\n            if (type(eval(df.iloc[0, x])) == type([])) & (df.columns[x] not in target):\n                lists_to_dump += [df.columns[x]]\n        except:\n            try:\n                if (type(df.iloc[0, x]) == type([])) & (df.columns[x] not in target):\n                    lists_to_dump += [df.columns[x]]\n            except:\n                pass\n\n    return lists_to_dump\n\n\ndef drop_const(df,target):\n    for col in df.columns:\n        if (df[col].astype(str).nunique(dropna=False) == 1) & (col not in target):\n            df = df.drop(col, axis=1)\n\n    return df\n\n\ndef drop_uniques(df,target):\n    for col in df.columns:\n        if (df[col].astype(str).nunique(dropna=False) == df.shape[0]) & (col not in target):\n            df = df.drop(col, axis=1)\n\n    return df\n\ndef load_df(csv_path,n_rows,target):\n    JSON_COLUMNS = [\"device\", \"geoNetwork\", \"totals\", \"trafficSource\"]\n\n    df = pd.DataFrame()\n\n    if n_rows == -1:\n        df = pd.read_csv(csv_path,\n                         converters={column: json.loads for column in JSON_COLUMNS}\n                         , dtype={\"fullVisitorId\": \"str\"}\n                         )\n    else:\n        df = pd.read_csv(csv_path,\n                         converters={column: json.loads for column in JSON_COLUMNS}\n                         ,nrows = n_rows, dtype={\"fullVisitorId\": \"str\"}\n                        )\n    \n    df = load_jsons_as_cols(df,JSON_COLUMNS)\n    \n    #while ((len(find_dicts(df,target)) > 0) | (len(find_lists(df,target)) > 0)):\n\n    if len(find_lists(df,target)) > 0:\n        for col in find_lists(df,target):\n            df[col] = unpack(df[col])\n            \n    while len(find_dicts(df,target)) > 0:\n        df = unfold(df, find_dicts(df,target),target)\n        \n    '''summa=0\n    new_col = []\n    #ciclo le colonne che ho scoperto contenere liste\n    for col in find_lists(df,target):\n        #ciclo le righe che contenti liste\n        for row in df[col]:\n            #ciclo la lista contenete dizionari\n            if len(row)>0:\n                #print(row[:3],len(row))\n                for d in row:\n                    try:\n                        summa += float(d['productPrice'])\n                    except:\n                        pass\n                new_col += [summa]\n                summa = 0\n    df[\"transactionRevenue\"] = pd.Series(new_col)'''\n    \n    df = drop_const(df,target)\n\n    df = drop_uniques(df,target)\n    \n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    \n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "24c5e17bf9c0411de1e7acf67f5c56a6ce20cbdc"
      },
      "cell_type": "markdown",
      "source": "## Kernel"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "695521031f87ac5ad55d656b66fd394cce6fc9fb"
      },
      "cell_type": "code",
      "source": "#eccezioni, non da droppare\ntarget = [\"fullVisitorId\",\"totals.totalTransactionRevenue\"]\n#num cols\nnums = [\"visitStartTime\",\"totals.totalTransactionRevenue\"]\n#cat cols\ncats = [\"trafficSource.adwordsClickInfo.gclId\",\"trafficSource.referralPath\",\"trafficSource.adContent\"]\n\nparameters = {\n    #numero massimo di valori in una singola colonna per essere flattata, altrimenti drop\n    \"max_new_feat\":5,\n    #inviare a kaggle tramite l' API\n    \"commit\":0,\n    #lgbm tuning parameters\n    \"n_leaves\" : 512,\n    \"feature_fraction\" : 0.99,\n    \"bagging_fraction\" : 0.99,\n    \"learn_rate\" : 0.004,\n    #train rows\n    \"train_rows\" : 10000,\n    #test_rows, per submittare deve essere settato a -1\n    \"test_rows\" : 500,\n    #il metodo principale è lgbm ma si può testare anche la regressione lineare\n    \"test_also_lin_reg\" : 1,\n    #bagging frequency\n    \"bagging_freq\" : 1,\n    #transactionRevenue\n    \"transactionRevenue\" : 0,\n    #percentuale di dev e val\n    \"percentage\" : 18,\n    #grouping_mode_cats\n    \"grouping_mode_cats\" : \"mean\",\n    #score\n    \"final_score\" : -1,\n    #minio di alberi per il rf\n    \"min_child_samples\" : -1\n}\n\nlocals().update(parameters)",
      "execution_count": 50,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "59bd38f057c65fcad3f3c5b35218c8b7f447a5c8"
      },
      "cell_type": "code",
      "source": "if (commit==1) & (test_rows != -1):\n    raise Exception(\"per submittare devi usare tutte le righe del test\")",
      "execution_count": 51,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "51c3b0c0df1a21ac5129b3f13d06bdf2afd1a498"
      },
      "cell_type": "code",
      "source": "%time train_df = load_df(train_file,train_rows,target)",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Loaded train_v2.csv. Shape: (10000, 92)\nCPU times: user 22.9 s, sys: 384 ms, total: 23.3 s\nWall time: 23.3 s\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "74ce9cce8d3dc7f613bae76a4ea0ee62aea410ba"
      },
      "cell_type": "code",
      "source": "%time test_df = load_df(test_file,test_rows,target)",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Loaded test_v2.csv. Shape: (500, 76)\nCPU times: user 2.64 s, sys: 4 ms, total: 2.65 s\nWall time: 2.65 s\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e74cb8eae9f2883cc3fbe9888d5119a2df320fd6"
      },
      "cell_type": "code",
      "source": "if \"totals.totalTransactionRevenue\" in test_df.columns:\n    test_df = test_df.drop(\"totals.totalTransactionRevenue\",axis=1)",
      "execution_count": 54,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "052c9d5e223ab28f066bb30c0c1ba4541d9d0483"
      },
      "cell_type": "code",
      "source": "if not transactionRevenue:\n    if \"totals.transactionRevenue\" in test_df.columns:\n        test_df = test_df.drop(\"totals.transactionRevenue\",axis=1)",
      "execution_count": 55,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bd887b3cabc953072fcca97dcfe95f04168e4eba"
      },
      "cell_type": "code",
      "source": "if not transactionRevenue:\n    if \"totals.transactionRevenue\" in train_df.columns:\n        train_df = train_df.drop(\"totals.transactionRevenue\",axis=1)",
      "execution_count": 56,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c5dac95148478e63411f223fb8829d00ce024d89"
      },
      "cell_type": "code",
      "source": "#controllare che nel nuovo test non droppi totalsRevenue perchè c'è solo nel train\ntrain_df,test_df = drop_uncommons(train_df,test_df,target)\n\nnums = find_num_cols(train_df,target,cats,nums)\n\ncats = find_cat_cols(train_df,target,nums)",
      "execution_count": 57,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d923a8856e34bdc78eefe3e2c4115f27ddef2ae"
      },
      "cell_type": "code",
      "source": "cc(train_df,test_df,cats,nums)",
      "execution_count": 58,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2ac12882a4990b040eec4dc519c0f1204cef2b5f"
      },
      "cell_type": "code",
      "source": "for col in nums:\n    if (col not in target) | (col==\"totals.totalTransactionRevenue\"):\n        train_df[col] = train_df[col].astype(float)\n        if col!=\"totals.totalTransactionRevenue\":\n            test_df[col] = test_df[col].astype(float)",
      "execution_count": 59,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "51c70d0477a12f3bad106bcfb82eb557b279a24c"
      },
      "cell_type": "code",
      "source": "# Impute 0 for missing target values\ntrain_df.fillna(0,inplace=True)\ntest_df.fillna(0,inplace=True)",
      "execution_count": 60,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8b004235f4977e8c82d1c57c2c50bacf149da61c"
      },
      "cell_type": "code",
      "source": "train_df = stringify_cats(train_df,cats)\ntest_df = stringify_cats(test_df,cats)",
      "execution_count": 61,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f1f82067671700fcf2aa46bde6443c927e1660ba"
      },
      "cell_type": "code",
      "source": "#droppo le colonne che hanno troppa varianza\ntrain_df,test_df,cats = drop_exceeding(train_df,test_df,max_new_feat,cats,target)",
      "execution_count": 62,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3922e5161efb158be0ec04c371d583ada2d10c1c"
      },
      "cell_type": "code",
      "source": "cc(train_df,test_df,cats,nums)",
      "execution_count": 63,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0e9306e925286d2f8879bd77b768e732abe38195"
      },
      "cell_type": "code",
      "source": "train_df[cats] = train_df[cats].astype(str)\ntest_df[cats] = test_df[cats].astype(str)",
      "execution_count": 64,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8f5c522824ba4956147f6f37ce1a71db46986e59"
      },
      "cell_type": "code",
      "source": "######QUI aggiungo il weekday\n\n#train_df[\"date\"].weekday()",
      "execution_count": 65,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a02753b1a3fee8ada0722ccefa6197a5da7ce422"
      },
      "cell_type": "code",
      "source": "%time train_df,test_df,cats = encode_cats(train_df,test_df,cats)",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": "\r\r\r\r\r\r\r\r\r\r\r[*          ]\r\r\r\r\r\r\r\r\r\r\r[**         ]",
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['trafficSource.adwordsClickInfo.page'\\n 'trafficSource.adwordsClickInfo.adNetworkType'\\n 'trafficSource.adwordsClickInfo.slot'\\n 'trafficSource.adwordsClickInfo.isVideoAd'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-2469594661be>\u001b[0m in \u001b[0;36mencode_cats\u001b[0;34m(train, test, cats)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# encodo e prendo i nomi vecchi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mnew_cols_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moh_enc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0mfeat_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moh_enc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2724\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2725\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2726\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2727\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1327\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['trafficSource.adwordsClickInfo.page'\\n 'trafficSource.adwordsClickInfo.adNetworkType'\\n 'trafficSource.adwordsClickInfo.slot'\\n 'trafficSource.adwordsClickInfo.isVideoAd'] not in index\""
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5327626cc3fbfd64b36dbb25f5cc396301dd19e"
      },
      "cell_type": "code",
      "source": "cc(train_df,test_df,cats,nums)",
      "execution_count": 67,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "403321e838b789fc87e6b4132fe2592a94327d02"
      },
      "cell_type": "code",
      "source": "%time train_df = group_me(train_df,\"fullVisitorId\",cats,nums,grouping_mode_cats)",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": "\r\r\r\r\r\r[*     ]\r\r\r\r\r\r[**    ]\r\r\r\r\r\r[***   ]",
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "DataError",
          "evalue": "No numeric types to aggregate",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDataError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-2469594661be>\u001b[0m in \u001b[0;36mgroup_me\u001b[0;34m(df, groupper, cats, nums, mode)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mdf_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroupped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdf_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcats\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroupper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"median\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mdf_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcats\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroupper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[0mnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_groupby_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'numeric_only'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cython_agg_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1307\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGroupByError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[1;32m   3970\u001b[0m                             min_count=-1):\n\u001b[1;32m   3971\u001b[0m         new_items, new_blocks = self._cython_agg_blocks(\n\u001b[0;32m-> 3972\u001b[0;31m             how, alt=alt, numeric_only=numeric_only, min_count=min_count)\n\u001b[0m\u001b[1;32m   3973\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_agged_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_cython_agg_blocks\u001b[0;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[1;32m   4042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4044\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No numeric types to aggregate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4046\u001b[0m         \u001b[0;31m# reset the locs in the blocks to correspond to our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDataError\u001b[0m: No numeric types to aggregate"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5ec9401f528265a6f7a5af8156a614f94cde75a8"
      },
      "cell_type": "code",
      "source": "%time test_df = group_me(test_df,\"fullVisitorId\",cats,nums,grouping_mode_cats)",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": "\r\r\r\r\r\r[*     ]\r\r\r\r\r\r[**    ]\r\r\r\r\r\r[***   ]",
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "DataError",
          "evalue": "No numeric types to aggregate",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDataError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-2469594661be>\u001b[0m in \u001b[0;36mgroup_me\u001b[0;34m(df, groupper, cats, nums, mode)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mdf_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroupped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdf_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcats\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroupper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"median\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mdf_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcats\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroupper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[0mnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_groupby_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'numeric_only'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cython_agg_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1307\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGroupByError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[1;32m   3970\u001b[0m                             min_count=-1):\n\u001b[1;32m   3971\u001b[0m         new_items, new_blocks = self._cython_agg_blocks(\n\u001b[0;32m-> 3972\u001b[0;31m             how, alt=alt, numeric_only=numeric_only, min_count=min_count)\n\u001b[0m\u001b[1;32m   3973\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_agged_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_cython_agg_blocks\u001b[0;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[1;32m   4042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4044\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No numeric types to aggregate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4046\u001b[0m         \u001b[0;31m# reset the locs in the blocks to correspond to our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDataError\u001b[0m: No numeric types to aggregate"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5f6d7b48d8e43e86b88c3bf7b4f6e3192d8d4af0"
      },
      "cell_type": "code",
      "source": "cc(train_df,test_df,cats,nums)",
      "execution_count": 70,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73a7b8e75fdd6739b873c24a68d9286407be33d5"
      },
      "cell_type": "code",
      "source": "load = -1\nbase = \"./saved_conf/\"\n\nif load == 1:\n    print(\"hai scelto di importare il dataset da disco\")\n    train_df = pd.read_csv(base + \"dump_train\")\n    test_df = pd.read_csv(base + \"dump_test\")\n    with open(base + \"dump_parameters\", 'r') as file:\n        file.read(json.loads(parameters))\n    locals().update(parameters)\nelse:\n    if load == 0:\n        print(\"hai scelto di scrivere il dataset su disco\")\n        %time train_df.to_csv(path_or_buf=base + \"dump_train\", header=True, mode='w',index=False)\n        test_df.to_csv(path_or_buf=base + \"dump_test\", header=True, mode='w',index=False)\n        with open(base + \"dump_parameters\", 'w') as file:\n            file.write(json.dumps(parameters))\n    else:\n        print(\"hai scelto di non caricare nè scaricare il dataset\")",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": "hai scelto di non caricare nè scaricare il dataset\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8b2c2d5fc7eae6329ab4d8976ca707921b4b2c8a"
      },
      "cell_type": "code",
      "source": "train_id = train_df[\"fullVisitorId\"].values\ntest_id = test_df[\"fullVisitorId\"].values",
      "execution_count": 72,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a87540f9ddeac96cc1f3e3df739dbb19032c7da9"
      },
      "cell_type": "code",
      "source": "#pulizia delle colonne con nomi assurdi\n#questa operazione può essere fatta in maniera safe perchè \n#a questo punto i due datasets hanno le stesse colonne con gli stessi nomi",
      "execution_count": 73,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "13fd08acacabe29bb6c7657fefd5a8ce83ca9555"
      },
      "cell_type": "code",
      "source": "train_df.columns = [col[:30] for col in train_df.columns]\ntest_df.columns = [col[:30] for col in test_df.columns]",
      "execution_count": 74,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1768d15c77c1f4c91addab401d9984ea880c7ad6"
      },
      "cell_type": "code",
      "source": "common_feats = list((set(train_df.columns).intersection(set(test_df.columns))).difference(set(target)))",
      "execution_count": 75,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4c3d771e25965c2d54f79abfe49957ef61423864"
      },
      "cell_type": "code",
      "source": "#qui viene bloccato il controllo di coerenza poichè le colonne cambiano, in particolare i nomi \n#vengono accorciati ma è safe farlo perchè i nomi delle colonne sono importanti solo nel preprocessing",
      "execution_count": 76,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3a27e6c74a92dd9817baa455ff04516f9ce27aa1"
      },
      "cell_type": "code",
      "source": "#cc(train_df,test_df,cats,nums)",
      "execution_count": 77,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "44d8e0b35bdef6e0af8d077208ec088dd9c681bb"
      },
      "cell_type": "code",
      "source": "#train_df.head()",
      "execution_count": 78,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "63780e47cc6d5586d3ff25fdda5fcaf1bb46a13b"
      },
      "cell_type": "code",
      "source": "#splitto il dataframe in development e validation ma cercando di mantenere in maniera \n#corretta il rapporto dei compratori che è circa dell' 1%\n\n#posso fare confronti con 0 perchè prima tutte le colonne sono state messe a 0 perciò non rischio NA\n\n#divido il train in 2 parti: quelli che hanno speso che sono l' 1% e quelli che non hanno \n#speso 99% e da ognuno estraggo il tot% quindi mantengo il rapporto tra i due\ntrain_money = train_df[train_df[\"totals.totalTransactionRevenue\"]>0]\ntrain_no_money = train_df[train_df[\"totals.totalTransactionRevenue\"]==0]\n\npercent = int(len(train_money)*percentage/100)\ntrain_money_val = train_money.iloc[:percent,]\n\ndev_df = train_money.iloc[percent:len(train_money),]\nval_df = train_money_val\n\npercent = int(len(train_no_money)*percentage/100)\ntrain_no_money_val = train_no_money.iloc[:percent,]\n\ndev_df = dev_df.append(train_no_money.iloc[percent:len(train_no_money),])\nval_df = val_df.append(train_no_money_val )\n\n\n###############################################\n#voglio lavorare su un subset perciò provo a ridurre la grandezza\n#mantengo il rapporto ma perdo info nelle features\n#quantity=1\n\n#dev_df=dev_df.iloc[:int(len(dev_df)*quantity),:]\n#val_df=val_df.iloc[:int(len(val_df)*quantity),:]\n###############################################\n\n#dev_y contiene la colonna addestramento in dev già log1p\ndev_y = np.log1p(dev_df[\"totals.totalTransactionRevenue\"].values)\n#val_y contiene la colonna target in val già log1p\nval_y = np.log1p(val_df[\"totals.totalTransactionRevenue\"].values)\n\n#dev_x contiene colonne numeriche e cat senza transRev\ndev_X = dev_df[ common_feats ]\n#val_x contiene colonne numeriche e cat senza transRev\nval_X = val_df[ common_feats ]\n#test è ciò che dobbiamo trovare\ntest_X = test_df[ common_feats ] ",
      "execution_count": 79,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b52b47fff35c0c0e32dfa46d8fcc44c0f0a753c9"
      },
      "cell_type": "code",
      "source": "def write(tipo):\n    parameters[\"final_score\"] = final_score\n    try:\n        if len(pd.read_csv(\"./tests.csv\").columns) != len(parameters.keys())+1:\n            print(\"il file tests.csv contiene meno colonne del necessario, verrà sostituito con perdita\")\n            !rm \"./tests.csv\"\n    except:\n        print(\"il file tests.csv verrà creato ora perchè non esistente\")\n    with open(\"./tests.csv\",'a') as ff:\n        if os.fstat(ff.fileno()).st_size == 0:\n            for k in parameters.keys():\n                print(k+',',file = ff,sep='',end='' )\n            print(\"type\",file = ff) \n        for v in parameters.values():\n            print(str(v)+',',file = ff,sep='',end='' )\n        print(tipo,file = ff ) ",
      "execution_count": 80,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1f2b96dc6709882a0e0488b049d79eee68f7f94b"
      },
      "cell_type": "markdown",
      "source": "## Linear Regression"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "0be8e47b39d632e5479f0fbe4f586a691150643e"
      },
      "cell_type": "code",
      "source": "if test_also_lin_reg == 1:\n    pred_test = lin(dev_X,dev_y,test_X)\n    pred_val = lin(dev_X,dev_y,val_X)\n\n    val_pred_df = pd.DataFrame({\"fullVisitorId\":val_df[\"fullVisitorId\"].values})\n    val_pred_df[\"totals.totalTransactionRevenue\"] = val_df[\"totals.totalTransactionRevenue\"].values\n    val_pred_df[\"PredictedRevenue\"] = np.expm1(pred_val)\n    val_pred_df = val_pred_df.groupby(\"fullVisitorId\")[\"totals.totalTransactionRevenue\", \"PredictedRevenue\"].sum().reset_index()\n    val_pred_df[val_pred_df[\"PredictedRevenue\"]>10^20]=0\n\n    final_score = np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"totals.totalTransactionRevenue\"].values)\n                                                     , np.log1p(val_pred_df[\"PredictedRevenue\"].values)))\n    print(final_score)\n    write(\"lin_reg\")",
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: '(entrance)'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-b311bcfc8d24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtest_also_lin_reg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mpred_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mval_pred_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"fullVisitorId\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fullVisitorId\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-e56613b31d8b>\u001b[0m in \u001b[0;36mlin\u001b[0;34m(dev, val, test)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    349\u001b[0m         X = check_array(X, copy=self.copy, warn_on_dtype=True,\n\u001b[1;32m    350\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                         force_all_finite=\"allow-nan\")\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mdata_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '(entrance)'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "a7de02e01d711ffd432e6471a1e658202618bf21"
      },
      "cell_type": "markdown",
      "source": "## LightGBM single-tree"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "86a121a1a94db3f8c7812770288172ee30a603e7"
      },
      "cell_type": "code",
      "source": "# custom function to run light-gbm model\ndef lgbm(train_X, train_y, val_X, val_y, test_X):\n    \n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : n_leaves,\n        \"feature_fraction\" : feature_fraction,\n        \"bagging_fraction\" : bagging_fraction,\n        \"bagging_freq\":bagging_freq,\n        \"learning_rate\" : learn_rate,\n        \"verbosity\" : -1\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    model = lgb.train(params, lgtrain, 10000, valid_sets=[lgval], early_stopping_rounds=300, verbose_eval=300,keep_training_booster = True )\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    pred_val_y = model.predict(val_X, num_iteration=model.best_iteration)\n    return pred_test_y, model, pred_val_y",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e93da60937bbfbd21275582d4226e5cc2fb3242b"
      },
      "cell_type": "code",
      "source": "#%time pred_test, model, pred_val = lgbm(dev_X, dev_y, val_X, val_y, test_X)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cfca505979895a46eeccf3441d21086bdac2724d"
      },
      "cell_type": "code",
      "source": "def score():\n    pred_val[pred_val<0] = 0\n    val_pred_df = pd.DataFrame({\"fullVisitorId\":val_df[\"fullVisitorId\"].values})\n    val_pred_df[\"totals.totalTransactionRevenue\"] = val_df[\"totals.totalTransactionRevenue\"].values\n    val_pred_df[\"PredictedRevenue\"] = np.expm1(pred_val)\n    val_pred_df = val_pred_df.groupby(\"fullVisitorId\")[\"totals.totalTransactionRevenue\", \"PredictedRevenue\"].sum().reset_index()\n    final_score = np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"totals.totalTransactionRevenue\"].values), np.log1p(val_pred_df[\"PredictedRevenue\"].values)))\n    print(final_score)\n\n    sub_df = pd.DataFrame({\"fullVisitorId\":test_id})\n    pred_test[pred_test<0] = 0\n    sub_df[\"PredictedLogRevenue\"] = np.expm1(pred_test)\n    sub_df = sub_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\n    sub_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\n    sub_df[\"PredictedLogRevenue\"] = np.log1p(sub_df[\"PredictedLogRevenue\"])\n    \n    \n    write(\"LightGBM\")\n    \n    if commit:\n        !kaggle competitions submit -c ga-customer-revenue-prediction -f {my_submission_file} -m \"No Message\"\n    \n    \n    return final_score,sub_df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0bc85d10025faa400292c31352935d1a53c6ae0b"
      },
      "cell_type": "code",
      "source": "def write_df(sub_df):\n    sub_df.to_csv(path_or_buf=my_submission_file, header=True, mode='w',index=False)\n    !wc -l {my_submission_file}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "635b2ae630367d711c034a788a90f93305c753ed"
      },
      "cell_type": "code",
      "source": "#final_score,sub_df = score()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc2f9ab61e0a0eca4e80c46b9c5be96b44eed546"
      },
      "cell_type": "code",
      "source": "#write_df(sub_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7cec00c575dce7f40eca246d8128b125b8dbe356"
      },
      "cell_type": "code",
      "source": "def plot_imp(model):\n    fig, ax = plt.subplots(figsize=(12,18))\n    lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n    ax.grid(False)\n    plt.title(\"LightGBM - Feature Importance\", fontsize=15)\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e93a969a334ae51edc668b93f495f37043e4fdb2"
      },
      "cell_type": "code",
      "source": "#plot_imp(model)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8732df7388d457b7315a77b74a1bffebf1443e69"
      },
      "cell_type": "markdown",
      "source": "## LightGBM con rf"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0e95cfc0c6737ebe77dd3ddc67bf7d0d460f3328"
      },
      "cell_type": "code",
      "source": "parameters[\"n_leaves\"] = 400\nparameters[\"bagging_fraction\"] = 0.99\nparameters[\"feature_fraction\"] = 0.99\nparameters[\"bagging_freq\"] = 20\nparameters[\"min_child_samples\"] = 10\nlocals().update(parameters)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8ad01245d2450b338b3be128943861af91087141"
      },
      "cell_type": "code",
      "source": "# custom function to run light-gbm model\ndef lgbm_rf(train_X, train_y, val_X, val_y, test_X):\n    \n    params = {\n        \"objective\" : \"regression\", \n        \"metric\" : \"rmse\",\n        \"num_leaves\" : n_leaves,\n        \"learning_rate\" : learn_rate,\n        \"bagging_fraction\" : bagging_fraction,\n        \"feature_fraction\" : feature_fraction,\n        \"bagging_freq\":bagging_freq,\n        'max_depth':-1,\n        \"min_child_samples\" : min_child_samples,\n        \"boosting\":\"rf\"\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    model = lgb.train(params, lgtrain, 3000, valid_sets=[lgval], verbose_eval=500,keep_training_booster = True )\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    pred_val_y = model.predict(val_X, num_iteration=model.best_iteration)\n    return pred_test_y, model, pred_val_y",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f8f2e4684fb6d0a440b4adfef2fb474c4ed21ea4"
      },
      "cell_type": "code",
      "source": "#%time pred_test, model, pred_val = lgbm_rf(dev_X, dev_y, val_X, val_y, test_X)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7aa61ab60e827ed4a3b1b4f6e643b40425e0a8f6"
      },
      "cell_type": "code",
      "source": "#final_score,sub_df = score()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7d0333631d19b5652d31d3c64c4577c3ab2221ba"
      },
      "cell_type": "code",
      "source": "#plot_imp(model)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dc26a5d7c9674a0767785e90e9d724cfe28495f7"
      },
      "cell_type": "markdown",
      "source": "## Iterative Testing"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "824062cff84ef8ce63d2b5c1ddaec09ccf12b942"
      },
      "cell_type": "code",
      "source": "def test_leaves_lgbm():\n    for x in [x for x in range(4,10)]:\n        x = 2**x\n        print(\"testing for \",x)\n        parameters[\"n_leaves\"] = x\n        locals().update(parameters)\n        %time pred_test, model, pred_val = lgbm(dev_X, dev_y, val_X, val_y, test_X)\n        final_score,sub_df = score()\n        print(\"---------------\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "40fbe57f26e6ee0ca89c0d0aa04c3b7948137077"
      },
      "cell_type": "code",
      "source": "%time test_leaves_lgbm()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f4d3f2b2171efddba2d854ae3e01284a09a25d23"
      },
      "cell_type": "code",
      "source": "def test_leaves_lgbm_rf():   \n    for x in [2**x for x in range(4,10)]:\n        print(\"testing for \",x)\n        parameters[\"n_leaves\"] = x\n        locals().update(parameters)\n        %time pred_test, model, pred_val = lgbm_rf(dev_X, dev_y, val_X, val_y, test_X)\n        final_score,sub_df = score()\n        print(\"---------------\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "64666a29302b0fff8edf5a93ad593f5358eed8e3"
      },
      "cell_type": "code",
      "source": "%time test_leaves_lgbm_rf()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a15f322d8598d01c1b8126026520e9611c087b91"
      },
      "cell_type": "code",
      "source": "def test_leaves_lgbm_best_value():   \n    for x in [x for x in range(16,33)]:\n        print(\"testing for \",x)\n        parameters[\"n_leaves\"] = x\n        locals().update(parameters)\n        %time pred_test, model, pred_val = lgbm(dev_X, dev_y, val_X, val_y, test_X)\n        final_score,sub_df = score()\n        print(\"---------------\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a2d50cc1f14844691ac2e3e75ea9c6d58fd8cb78"
      },
      "cell_type": "code",
      "source": "%time test_leaves_lgbm_best_value()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "53ab6bfd63ba76a607013b8e4c7c9f10ed79d6d3"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}